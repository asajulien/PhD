{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cfd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from IPython.display import display\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9153a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568868bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Station coordinates (FLATS around stations?)\n",
    "\n",
    "stazioni = pd.read_csv('C:/Users/arj26323/Documents/Data/NERR data/All_stations2.csv')\n",
    "\n",
    "# print(stazioni['Station Code'].unique())\n",
    "\n",
    "stazioni = stazioni[stazioni['Station Code'].str.contains('nut')]\n",
    "\n",
    "stazioni.rename(columns={\"Latitude \": \"Latitude\", \" Longitude\": \"Longitude\"}, inplace= True)\n",
    "\n",
    "dfs = stazioni[['Station Code', 'Station Name', 'Latitude', 'Longitude']].reset_index()\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e98413",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = dfs[dfs['Station Code']=='cbmrrnut  '] #two spaces put in after each code for some stupid fucking reason\n",
    "\n",
    "# dfs.iloc[0,0]\n",
    "\n",
    "dfx\n",
    "\n",
    "# dfx.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = ee.Geometry.Point([dfx.iloc[0,4], dfx.iloc[0,3]])\n",
    "#lon and lat\n",
    "\n",
    "Map = geemap.Map(center=[38.7813,-76.7137], zoom=16)\n",
    "\n",
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to cloud mask from the pixel_qa band of Landsat 5/8 SR data.\n",
    "def maskL5sr(image):\n",
    "    qaMask = image.select('QA_PIXEL').bitwiseAnd(int('11111', 2)).eq(0)\n",
    "    saturationMask = image.select('QA_RADSAT').eq(0)\n",
    "    # Apply the scaling factors to the appropriate bands.\n",
    "    opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "    thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n",
    "    # Replace the original bands with the scaled ones and apply the masks.\n",
    "    return image.addBands(opticalBands, None, True) \\\n",
    "    .addBands(thermalBands, None, True) \\\n",
    "    .updateMask(qaMask) \\\n",
    "    .updateMask(saturationMask)\n",
    "\n",
    "def addFLATSL7(image):\n",
    "    flats = ee.Image(0).expression(\n",
    "        '1/(1+2.718281828459045**-(1.51 + 12.5*(RED-SWIR)/(RED+SWIR) - 41.2*(NIR-RED)/(NIR+6*RED-7.5*BLUE+1)))', {\n",
    "            'SWIR': image.select('SR_B5'),\n",
    "            'NIR': image.select('SR_B4'),\n",
    "            'RED': image.select('SR_B3'),\n",
    "            'BLUE': image.select('SR_B1')\n",
    "        })\n",
    "    \n",
    "    return image.addBands(flats.rename('flats'))\n",
    "\n",
    "def addFLATSL5(image):\n",
    "    flats = ee.Image(0).expression(\n",
    "        '1/(1+2.718281828459045**-(1.51 + 12.5*(0.972*(RED-SWIR)/(RED+SWIR)-0.008) - 41.2*(0.991*(NIR-RED)/(NIR+6*RED-7.5*BLUE+1)-0.0014)))', {\n",
    "            'SWIR': image.select('SR_B5'),\n",
    "            'NIR': image.select('SR_B4'),\n",
    "            'RED': image.select('SR_B3'),\n",
    "            'BLUE': image.select('SR_B1')\n",
    "        })\n",
    "    \n",
    "    return image.addBands(flats.rename('flats'))\n",
    "\n",
    "\n",
    "def addFLATSL8(image):\n",
    "    flats = ee.Image(0).expression(\n",
    "        '1/(1+2.718281828459045**-(1.51 + 12.5*(0.841*(RED-SWIR)/(RED+SWIR) - 0.019) - 41.2*(0.771*(NIR-RED)/(NIR+6*RED-7.5*BLUE+1) + 0.011)))', {\n",
    "            'SWIR': image.select('SR_B6'),\n",
    "            'NIR': image.select('SR_B5'),\n",
    "            'RED': image.select('SR_B4'),\n",
    "            'BLUE': image.select('SR_B2')\n",
    "        })\n",
    "    \n",
    "    return image.addBands(flats.rename('flats'))\n",
    "\n",
    "def addFLATSL9(image):\n",
    "    flats = ee.Image(0).expression(\n",
    "        '1/(1+2.718281828459045**-(1.51 + 12.5*(1.225*(RED-SWIR)/(RED+SWIR) + 0.096) - 41.2*(1.038* (NIR-RED)/(NIR+6*RED-7.5*BLUE+1) - 0.004)))', {\n",
    "            'SWIR': image.select('SR_B6'),\n",
    "            'NIR': image.select('SR_B5'),\n",
    "            'RED': image.select('SR_B4'),\n",
    "            'BLUE': image.select('SR_B2')\n",
    "        })\n",
    "    \n",
    "    return image.addBands(flats.rename('flats'))\n",
    "\n",
    "def add_date_band(image):\n",
    "    # Get the acquisition date\n",
    "    date = ee.Date(image.get('system:time_start'))\n",
    "    \n",
    "    # Convert the date to milliseconds since Unix epoch\n",
    "    date_millis = date.millis()\n",
    "    \n",
    "    # Create an image with a single band representing the acquisition date\n",
    "    date_image = ee.Image.constant(date_millis).int64().rename('acquisition_date')\n",
    "    \n",
    "    # Add the date image as a band to the original image\n",
    "    return image.addBands(date_image)\n",
    "\n",
    "# vcr_lter = geemap.shp_to_ee('F:/Wetlands shapefiles/VCR domain/VCR_LTER_boundary.shp')\n",
    "\n",
    "#Loading gee datasets\n",
    "l8_col = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "    .filter(ee.Filter.calendarRange(1998, 2023,'year')) \\\n",
    "    .filterBounds(px.buffer(10000)) \\\n",
    "    .map(maskL5sr).map(addFLATSL8).map(add_date_band)\n",
    "\n",
    "l7_col = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "    .filter(ee.Filter.calendarRange(1984, 2023,'year')) \\\n",
    "    .filterBounds(px.buffer(10000))\\\n",
    "    .map(maskL5sr).map(addFLATSL7).map(add_date_band)\n",
    "\n",
    "l5_col = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "    .filter(ee.Filter.calendarRange(1984, 2023,'year')) \\\n",
    "    .filterBounds(px.buffer(10000))\\\n",
    "    .map(maskL5sr).map(addFLATSL5).map(add_date_band)\n",
    "\n",
    "l9_col = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\") \\\n",
    "    .filter(ee.Filter.calendarRange(1998, 2023,'year')) \\\n",
    "    .filterBounds(px.buffer(10000))\\\n",
    "    .map(maskL5sr).map(addFLATSL9).map(add_date_band)\n",
    "\n",
    "\n",
    "bb = ee.Geometry.Polygon([[-81.29, 31.425], [-81.29, 31.46], [-81.27, 31.46],[-81.27, 31.425]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_col = l5_col.merge(l7_col).merge(l8_col).merge(l9_col)\n",
    "\n",
    "# ultra = ultra_col.filter(ee.Filter.calendarRange(1984, 2023,'year')).filterBounds(flux_ga)\n",
    "ultra = ultra_col.filter(ee.Filter.calendarRange(2004, 2024,'year')).map(lambda image: image.clip(px)) \\\n",
    "    .filter(ee.Filter.lte('CLOUD_COVER_LAND', 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e674fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_extract(image):\n",
    "    \n",
    "    feature = image.sampleRegions(\n",
    "        collection=px,\n",
    "        scale=30,\n",
    "        geometries=True)\n",
    "    \n",
    "    return feature.limit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_list = ultra.toList(ultra.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract time series for nutrient station\n",
    "\n",
    "feature_list = ultra.map(point_extract).flatten().toList(ultra.size())\n",
    "values_list = []\n",
    "\n",
    "# Loop through the feature list and extract values\n",
    "for i in range(feature_list.size().getInfo()):\n",
    "    feature = ee.Feature(feature_list.get(i))\n",
    "    properties = feature.toDictionary()\n",
    "    values_list.append(properties.getInfo())\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "timeseries_df = pd.DataFrame(values_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(timeseries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "out_dir = os.path.expanduser('C:/Users/arj26323/Documents/Data/NERR data/')\n",
    "out_csv = os.path.join(out_dir, 'timeseries_VCR.csv')\n",
    "# timeseries_df.to_csv(out_csv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df = pd.read_csv('C:/Users/arj26323/Documents/Data/NERR data/timeseries_VCR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_df['Date'] = pd.to_datetime(timeseries_df['acquisition_date'], unit = 'ms')\n",
    "timeseries_df['DOY'] = timeseries_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "timeseries_df['DOY'] = pd.to_datetime(timeseries_df['DOY'])\n",
    "\n",
    "timeseries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data = timeseries_df, x = 'Date', y = 'flats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7199e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tides -- needs to be VA\n",
    "\n",
    "# path = r'C:/Users/arj26323/Documents/Data/flats/Tide data' \n",
    "# all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# li = []\n",
    "\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0)\n",
    "#     li.append(df)\n",
    "\n",
    "# df_tides = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# df2 = df_tides.copy()\n",
    "# df2['Time (GMT)'] = pd.to_datetime(df2['Time (GMT)'])\n",
    "# df2['DOY'] = pd.to_datetime(df2['Date'])\n",
    "# df_time = df2.copy()\n",
    "# df_time['Verified (m)'] = df_time['Verified (m)'].replace('-', float('nan')).astype('float')\n",
    "# df_time = df_time.dropna()\n",
    "# df_time['Tide change'] = df_time['Verified (m)'].diff()\n",
    "# df_time['Tide stage'] = df_time['Tide change'].apply(lambda x: 'Ebb' if x < 0 else 'Flood')\n",
    "\n",
    "# time_mask = (df_time['Time (GMT)'].dt.hour == 16)\n",
    "\n",
    "# df_time = df_time[time_mask]\n",
    "\n",
    "\n",
    "# ##Combine time/tide and flats data\n",
    "# df_combined = pd.merge(timeseries_df, df_time, on = 'DOY')\n",
    "# df_combined['Date'] = df_combined['Date_x']\n",
    "# df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dfy = df_combined[df_combined['Verified (m)'] < 0.3]\n",
    "\n",
    "sns.scatterplot(data = dfy, x = 'Date', y = 'flats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56296a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding nutrient data\n",
    "\n",
    "##SAP station\n",
    "path = r'C:/Users/arj26323/Documents/Data/NERR data/Files 2024' #changed from Files folder 4/25/24\n",
    "all_files = glob.glob(path + \"/sapdcnut*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df_sap = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df_sap)\n",
    "\n",
    "df_sap = pd.concat(li, axis=0, ignore_index=True).dropna(subset=['NO23F']).dropna(axis=1)\n",
    "\n",
    "##Begin:\n",
    "df_sap = df_sap[df_sap['NO23F'] < 0.9] ##removes several extremely large outliers\n",
    "df_sap['Date_m'] = pd.to_datetime(df_sap['DateTimeStamp'])\n",
    "df_sap['DOY'] = df_sap['Date_m'].dt.strftime('%Y-%m-%d')\n",
    "df_sap['DOY'] = pd.to_datetime(df_sap['DOY'])\n",
    "\n",
    "df_sap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc91e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df_sap, x = 'Date_m', y = 'NO23F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5086b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine nutrient data with flats and time/tide data\n",
    "##Need to do merge_asof to take advantage of the large sample size for nutrient data\n",
    "##Also, need to do a little pixel neighborhood, rather than a single pixel when analyzing FLATS/biomass (eventually)\n",
    "\n",
    "\n",
    "# df_combined.columns = df_combined.columns.str.rstrip('_x') #remove suffix to avoid error\n",
    "# df_combined2 = pd.merge(df_combined, df_sap, on = 'DOY')\n",
    "# # df_combined2['Date'] = df_combined2['Date_']\n",
    "# df_combined2.rename(columns={ df_combined2.columns[-1]: \"Date_xxx\" }, inplace = True)\n",
    "\n",
    "\n",
    "##Merge, to nearest Landsat acquisition\n",
    "df_combined2 = pd.merge_asof(\n",
    "    df_combined.sort_values('Date'), \n",
    "    df_sap.sort_values('Date_m'), \n",
    "    left_on = 'Date',\n",
    "    right_on = 'Date_m', \n",
    "    direction = 'nearest', \n",
    "    tolerance=pd.Timedelta(\"1210000000ms\") ##Two weeks - is this ok?\n",
    ")\n",
    "\n",
    "df_combined2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07aacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_combined2['NO23F'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "dfz = df_combined2[df_combined2['Verified (m)'] < 0.3]\n",
    "\n",
    "sns.scatterplot(data = dfz, x = 'NO23F', y = 'flats') ##This doesn't really tell you much. Need time + flats + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481edbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfz.shape)\n",
    "\n",
    "##Tide quantiles will be different\n",
    "\n",
    "max(dfz['Verified (m)'])\n",
    "\n",
    "# dfz.quantile([.25, .5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = dfz['flats'].corr(dfz['NO23F'])\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "##Time lag\n",
    "lag = 1 #why choose this number?\n",
    "\n",
    "no23_shifted = dfz['NO23F'].shift(-lag)\n",
    "\n",
    "no23_shifted = no23_shifted.dropna()\n",
    "\n",
    "# Calculate the correlation between the shifted variable and the original variable\n",
    "time_lagged_corr = dfz['flats'].corr(no23_shifted)\n",
    "\n",
    "# Print the time lagged correlation coefficient\n",
    "print(\"Time lagged correlation coefficient between 'NO23F' and 'flats' (lag = {}): {:.2f}\".format(lag, time_lagged_corr))\n",
    "\n",
    "sns.scatterplot(x = no23_shifted, y = dfz['flats']) ##This doesn't really tell you much. Need time + flats + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = dfz, x = 'Date', y = 'flats')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9831b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Primary y-axis (flats)\n",
    "sns.scatterplot(ax=ax1, data=dfz, x='Date', y='flats', label='Flats', color='blue')\n",
    "ax1.set_ylabel('Flats', color='blue', labelpad=10)\n",
    "\n",
    "# Secondary y-axis (NO23F)\n",
    "ax2 = ax1.twinx()  # Create a twin axis that shares the x-axis\n",
    "sns.scatterplot(ax=ax2, data=dfz, x='Date', y='NO23F', label='NO23F', color='red')\n",
    "ax2.set_ylabel('NO23F', color='red',  labelpad=15)\n",
    "\n",
    "# Customize the plot further (optional)\n",
    "# Place legend on primary y-axis\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, labels, loc='upper left')  \n",
    "\n",
    "# Alternatively, place legend on secondary y-axis\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(handles2, labels2, loc='upper right')\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gee] *",
   "language": "python",
   "name": "conda-env-gee-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
