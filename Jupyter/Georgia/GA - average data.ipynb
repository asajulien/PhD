{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b594e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from IPython.display import display\n",
    "import os\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as sm\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from math import sqrt\n",
    "import glob\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c0c8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000_B1</th>\n",
       "      <th>2000_B2</th>\n",
       "      <th>2000_B3</th>\n",
       "      <th>2000_B4</th>\n",
       "      <th>2000_B5</th>\n",
       "      <th>2000_B6</th>\n",
       "      <th>2000_B7</th>\n",
       "      <th>2001_B1</th>\n",
       "      <th>2001_B2</th>\n",
       "      <th>2001_B3</th>\n",
       "      <th>...</th>\n",
       "      <th>Red_band</th>\n",
       "      <th>NIR_band</th>\n",
       "      <th>SWIR1_band</th>\n",
       "      <th>SWIR2_band</th>\n",
       "      <th>savi</th>\n",
       "      <th>wdrvi5</th>\n",
       "      <th>nd_r_g</th>\n",
       "      <th>nd_g_b</th>\n",
       "      <th>nd_swir2_nir</th>\n",
       "      <th>nd_swir2_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.073647</td>\n",
       "      <td>-0.039700</td>\n",
       "      <td>-0.001439</td>\n",
       "      <td>0.085803</td>\n",
       "      <td>-0.316222</td>\n",
       "      <td>-0.020588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.096892</td>\n",
       "      <td>0.006386</td>\n",
       "      <td>-0.073810</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>-0.193939</td>\n",
       "      <td>0.155266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0748</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>0.089470</td>\n",
       "      <td>-0.007958</td>\n",
       "      <td>0.084165</td>\n",
       "      <td>-0.371648</td>\n",
       "      <td>0.045918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.180337</td>\n",
       "      <td>0.259516</td>\n",
       "      <td>0.011024</td>\n",
       "      <td>0.084629</td>\n",
       "      <td>-0.514563</td>\n",
       "      <td>0.043219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.138354</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>-0.048362</td>\n",
       "      <td>0.038640</td>\n",
       "      <td>-0.309436</td>\n",
       "      <td>0.035573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.177415</td>\n",
       "      <td>0.109128</td>\n",
       "      <td>-0.047702</td>\n",
       "      <td>0.134272</td>\n",
       "      <td>-0.380808</td>\n",
       "      <td>0.055077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>0.134286</td>\n",
       "      <td>0.117506</td>\n",
       "      <td>-0.055199</td>\n",
       "      <td>0.116848</td>\n",
       "      <td>-0.394166</td>\n",
       "      <td>0.047865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.118447</td>\n",
       "      <td>0.093054</td>\n",
       "      <td>-0.052055</td>\n",
       "      <td>0.131075</td>\n",
       "      <td>-0.467018</td>\n",
       "      <td>-0.066256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.101097</td>\n",
       "      <td>-0.024259</td>\n",
       "      <td>0.053215</td>\n",
       "      <td>0.126649</td>\n",
       "      <td>-0.205060</td>\n",
       "      <td>0.113806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.157871</td>\n",
       "      <td>0.129211</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.136598</td>\n",
       "      <td>-0.344105</td>\n",
       "      <td>0.117227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     2000_B1  2000_B2  2000_B3  2000_B4  2000_B5  2000_B6  2000_B7  2001_B1  \\\n",
       "0     0.0293   0.0348   0.0347   0.0641   0.0540   0.2906   0.0333   0.0286   \n",
       "1     0.0293   0.0348   0.0347   0.0641   0.0540   0.2906   0.0333   0.0286   \n",
       "2     0.0293   0.0348   0.0347   0.0641   0.0540   0.2906   0.0333   0.0286   \n",
       "3     0.0293   0.0348   0.0347   0.0641   0.0540   0.2906   0.0333   0.0286   \n",
       "4     0.0293   0.0348   0.0347   0.0641   0.0540   0.2906   0.0333   0.0286   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "203   0.0315   0.0394   0.0420   0.0906   0.0659   0.2915   0.0376   0.0282   \n",
       "204   0.0315   0.0394   0.0420   0.0906   0.0659   0.2915   0.0376   0.0282   \n",
       "205   0.0315   0.0394   0.0420   0.0906   0.0659   0.2915   0.0376   0.0282   \n",
       "206   0.0361   0.0487   0.0495   0.1084   0.1146   0.2920   0.0592   0.0306   \n",
       "207   0.0361   0.0487   0.0495   0.1084   0.1146   0.2920   0.0592   0.0306   \n",
       "\n",
       "     2001_B2  2001_B3  ...  Red_band  NIR_band  SWIR1_band  SWIR2_band  \\\n",
       "0     0.0451   0.0389  ...    0.0347    0.0641      0.0540      0.0333   \n",
       "1     0.0451   0.0389  ...    0.0389    0.0788      0.0844      0.0532   \n",
       "2     0.0451   0.0389  ...    0.0374    0.0895      0.0748      0.0410   \n",
       "3     0.0451   0.0389  ...    0.0321    0.1092      0.0647      0.0350   \n",
       "4     0.0451   0.0389  ...    0.0610    0.1242      0.1149      0.0655   \n",
       "..       ...      ...  ...       ...       ...         ...         ...   \n",
       "203   0.0401   0.0425  ...    0.0549    0.1367      0.1094      0.0613   \n",
       "204   0.0401   0.0425  ...    0.0368    0.0932      0.0731      0.0405   \n",
       "205   0.0401   0.0425  ...    0.0346    0.0834      0.0555      0.0303   \n",
       "206   0.0401   0.0425  ...    0.0475    0.0905      0.0928      0.0597   \n",
       "207   0.0401   0.0425  ...    0.0433    0.1123      0.1098      0.0548   \n",
       "\n",
       "         savi    wdrvi5    nd_r_g    nd_g_b  nd_swir2_nir  nd_swir2_r  \n",
       "0    0.073647 -0.039700 -0.001439  0.085803     -0.316222   -0.020588  \n",
       "1    0.096892  0.006386 -0.073810  0.223881     -0.193939    0.155266  \n",
       "2    0.124661  0.089470 -0.007958  0.084165     -0.371648    0.045918  \n",
       "3    0.180337  0.259516  0.011024  0.084629     -0.514563    0.043219  \n",
       "4    0.138354  0.008936 -0.048362  0.038640     -0.309436    0.035573  \n",
       "..        ...       ...       ...       ...           ...         ...  \n",
       "203  0.177415  0.109128 -0.047702  0.134272     -0.380808    0.055077  \n",
       "204  0.134286  0.117506 -0.055199  0.116848     -0.394166    0.047865  \n",
       "205  0.118447  0.093054 -0.052055  0.131075     -0.467018   -0.066256  \n",
       "206  0.101097 -0.024259  0.053215  0.126649     -0.205060    0.113806  \n",
       "207  0.157871  0.129211 -0.009153  0.136598     -0.344105    0.117227  \n",
       "\n",
       "[208 rows x 191 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Proof of concept\n",
    "os.chdir(r'C:/Users/arj26323/Documents/Data/Biomass datasets/Sapelo/')\n",
    "df = pd.read_csv('GA_spectra_FINAL_3-4-22.csv') ##very different results than GA_spectra\n",
    "\n",
    "##Variables from Byrd et al. 2018:\n",
    "df['savi'] = ((df['NIR_band']-df['Red_band'])*1.5)/(df['NIR_band']+df['Red_band']+0.5)\n",
    "df['wdrvi5'] = (0.5*df['NIR_band']-df['Red_band'])/(0.5*df['NIR_band']+df['Red_band'])\n",
    "df['nd_r_g'] = (df['Red_band']-df['Green_band'])/(df['Red_band']+df['Green_band'])\n",
    "df['nd_g_b'] = (df['Green_band']-df['Blue_band'])/(df['Green_band']+df['Blue_band'])\n",
    "df['nd_swir2_nir'] = (df['SWIR2_band']-df['NIR_band'])/(df['SWIR2_band']+df['NIR_band'])\n",
    "df['nd_swir2_r'] = (df['SWIR2_band']-df['Red_band'])/(df['SWIR2_band']+df['Red_band'])\n",
    "\n",
    "df\n",
    "\n",
    "# g = sns.scatterplot(x = 'NIR_band', y = 'Mean_Biomass', data = df)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547b2d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 401.15938130952384\n",
      "Mean Squared Error: 280242.7050352075\n",
      "Root Mean Squared Error: 529.3795472392256\n",
      "-0.24265859874974427\n",
      "-0.24265859874974427\n"
     ]
    }
   ],
   "source": [
    "##First go at machine learning, using the methods of Byrd et al 2018\n",
    "\n",
    "##Need (from table 3)=: SAVI, normalized difference green/blue index, wide dynamic range vegetation index 5, ndvi red/green, \\\n",
    "##  ndvi swir2/nir, ndvi swir2/red, site? (site may be important when combining GA, VA, and MA data)\n",
    "\n",
    "##Random Forest test! REMEMBER: by combining landsat 5 and 8 bands, you are grouping bands with slightly different wavelengths. \n",
    "##ONLY INDICES to account for this\n",
    "\n",
    "##From: https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
    "\n",
    "df = df[df['ndvi'].notna()] ##remove rows with NaN for columns used in the model \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xargs=df[['ndvi', 'nd_swir2_r', 'nd_swir2_nir', 'nd_g_b', 'nd_r_g', 'wdrvi5', 'savi']] \n",
    "yargs=df['Mean_Biomass'] \n",
    "\n",
    "size_x = 0.2\n",
    "seed = 0\n",
    "xargs_train, xargs_test, yargs_train, yargs_test = train_test_split(xargs, yargs, test_size=size_x, random_state = seed) \n",
    "## 80% training and 20% test\n",
    "\n",
    "## Feature Scaling - is this necessary?\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# xargs_train = sc.fit_transform(xargs_train)\n",
    "# xargs_test = sc.transform(xargs_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "##Create a Gaussian Classifier\n",
    "reg_x=RandomForestRegressor(n_estimators=100, random_state = 0) ##play with random state\n",
    "\n",
    "##Train the model using the training sets y_pred=clf.predict(xargs_test)\n",
    "reg_x.fit(xargs_train, yargs_train)\n",
    "y_pred=reg_x.predict(xargs_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(yargs_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(yargs_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(yargs_test, y_pred)))\n",
    "\n",
    "r_square = metrics.r2_score(yargs_test, y_pred)\n",
    "print(r_square) ##pretty low (0.29), all things considered\n",
    "print(reg_x.score(xargs_test, yargs_test)) ##same as above, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe0b8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     208.000000\n",
       "mean      663.391943\n",
       "std       478.262638\n",
       "min       106.366000\n",
       "25%       312.882500\n",
       "50%       476.735250\n",
       "75%       854.051500\n",
       "max      2452.424000\n",
       "Name: Mean_Biomass, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.scatter(xargs,yargs, color='red') ##plotting real points\n",
    "df['Mean_Biomass'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551e6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.09984929920335635"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(yargs_test, y_pred, color = 'blue') ##label axes - x,y in order?\n",
    "# plt.plot(X_val, regressor.predict(X_val), color = 'red')\n",
    "plt.show()\n",
    "\n",
    "np.corrcoef(yargs_test, y_pred)[0, 1]\n",
    "##the individual bands were better by themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a000a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:45:41] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Mean Absolute Error: 563.235899659864\n",
      "Mean Squared Error: 542753.3410860215\n",
      "Root Mean Squared Error: 736.717952194747\n",
      "-1.40668925250334\n",
      "-0.24265859874974427\n"
     ]
    }
   ],
   "source": [
    "##xgboost - seems to perform worse than random forest, at least with initial data\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## split data into X and y\n",
    "xargs=df[['ndvi', 'nd_swir2_r', 'nd_swir2_nir', 'nd_g_b', 'nd_r_g', 'wdrvi5', 'savi']] \n",
    "yargs=df['Mean_Biomass'] \n",
    "\n",
    "## split data into train and test sets\n",
    "size_x = 0.2\n",
    "seed = 0\n",
    "x_xg_train, x_xg_test, y_xg_train, y_xg_test = train_test_split(xargs, yargs, test_size=size_x, random_state = seed)\n",
    "\n",
    "##fit model no training data\n",
    "xg_reg = XGBClassifier()\n",
    "xg_reg.fit(x_xg_train, y_xg_train)\n",
    "\n",
    "##make predictions for test data\n",
    "y_xg_pred = xg_reg.predict(x_xg_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "\n",
    "##evaluate predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_xg_test, y_xg_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_xg_test, y_xg_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_xg_test, y_xg_pred)))\n",
    "\n",
    "r_square = metrics.r2_score(y_xg_test, y_xg_pred)\n",
    "print(r_square)\n",
    "print(reg_x.score(x_xg_test, y_xg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Major question: why is the averaged dataset random forest worse than the individual plots rf? Sample size?\n",
    "##Or is zonal_stats pixel extraction not what you think it is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
